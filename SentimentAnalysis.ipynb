{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7aVfuyvkgUt8"
      },
      "source": [
        "Install and Import Required Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yJcMvylV6o6n",
        "outputId": "48bdef5d-4da1-4e63-d97d-9499340ea729"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting snscrape\n",
            "  Downloading snscrape-0.3.4-py3-none-any.whl (35 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from snscrape) (4.6.3)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from snscrape) (4.2.6)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.7/dist-packages (from snscrape) (2.23.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->snscrape) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->snscrape) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->snscrape) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->snscrape) (3.0.4)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->snscrape) (1.7.1)\n",
            "Installing collected packages: snscrape\n",
            "Successfully installed snscrape-0.3.4\n",
            "Collecting yfinance\n",
            "  Downloading yfinance-0.1.70-py2.py3-none-any.whl (26 kB)\n",
            "Collecting requests>=2.26\n",
            "  Downloading requests-2.27.1-py2.py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/dist-packages (from yfinance) (1.21.6)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from yfinance) (0.0.10)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from yfinance) (1.3.5)\n",
            "Collecting lxml>=4.5.1\n",
            "  Downloading lxml-4.8.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (6.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.4 MB 8.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->yfinance) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->yfinance) (2022.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.24.0->yfinance) (1.15.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.26->yfinance) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.26->yfinance) (2021.10.8)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests>=2.26->yfinance) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.26->yfinance) (2.10)\n",
            "Installing collected packages: requests, lxml, yfinance\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: lxml\n",
            "    Found existing installation: lxml 4.2.6\n",
            "    Uninstalling lxml-4.2.6:\n",
            "      Successfully uninstalled lxml-4.2.6\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.27.1 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed lxml-4.8.0 requests-2.27.1 yfinance-0.1.70\n",
            "Collecting vaderSentiment\n",
            "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[K     |████████████████████████████████| 125 kB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from vaderSentiment) (2.27.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (1.24.3)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (2021.10.8)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (2.10)\n",
            "Installing collected packages: vaderSentiment\n",
            "Successfully installed vaderSentiment-3.3.2\n"
          ]
        }
      ],
      "source": [
        "!pip install snscrape\n",
        "!pip install yfinance\n",
        "!pip install vaderSentiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vEvJ-eDMVJUg",
        "outputId": "84901f7c-e96d-4b09-e311-759d14381821"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Utilities\n",
        "import re\n",
        "import string\n",
        "import datetime\n",
        "\n",
        "# Twitter Data\n",
        "import snscrape.modules.twitter as sntwitter\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "#Yahoo Finance\n",
        "import yfinance as yf\n",
        "\n",
        "#Text Pre-processing\n",
        "import nltk\n",
        "from nltk import word_tokenize, pos_tag\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "\n",
        "# Data Model\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ceolrDcPgwrn"
      },
      "source": [
        "Fetch Twitter Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xUurm6j8U4ah"
      },
      "outputs": [],
      "source": [
        "def get_tweets(about,since,until):\n",
        "\n",
        "  tweets_list = []\n",
        "\n",
        "  for i,tweet in enumerate(sntwitter.TwitterSearchScraper('{0} + since:{1} until:{2} --en -filter:links -filter:replies'.format(about,since,until)).get_items()):\n",
        "    tweets_list.append([tweet.date, tweet.id, tweet.content])\n",
        "\n",
        "  return pd.DataFrame(tweets_list, columns=['Datetime', 'Tweet Id', 'Text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhh1CQtBg6lu"
      },
      "source": [
        "Fetch Yahoo Finance Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NZU4v3Z566tS"
      },
      "outputs": [],
      "source": [
        "def get_yfinance_data(company):\n",
        "  \n",
        "  data = yf.Ticker(company).history(period='3y')\n",
        "\n",
        "  return pd.DataFrame(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-xz8uocg0hR"
      },
      "source": [
        "Text Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pMEKngM4637x"
      },
      "outputs": [],
      "source": [
        "def remove_punctuation(input_text):\n",
        "\n",
        "  remove = string.punctuation\n",
        "  remove = remove.replace(\".\", \"\") # don't remove period\n",
        "  pattern = r\"[{}]\".format(remove) # create the pattern\n",
        "  x = input_text.translate({ord(char): None for char in remove})\n",
        "  return \" \".join([word.strip(string.punctuation) for word in x.split()])\n",
        "\n",
        "def remove_stopwords(input_text):\n",
        "\n",
        "  stopwords_list = stopwords.words('english')\n",
        "  inclusion_list = []\n",
        "  exclusion_list = ['rt','na','none']\n",
        "  clean_words = [word for word in input_text.split() if (word not in stopwords_list or word in inclusion_list)] \n",
        "  clean_words = [word for word in clean_words if word not in exclusion_list]\n",
        "  return \" \".join(clean_words) \n",
        "\n",
        "def lemmatize_words(input_text):\n",
        "\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  word_list = nltk.word_tokenize(input_text)\n",
        "  return ' '.join([lemmatizer.lemmatize(w) for w in set(word_list)])\n",
        "\n",
        "def text_cleaning(input_text):\n",
        "  \n",
        "  x = input_text.lower()\n",
        "\n",
        "  x = re.sub(r'@\\w+', '', x)\n",
        "\n",
        "  x = re.sub(r'-', ' ', x)\n",
        "\n",
        "  x = re.sub(r'http.?://[^\\s]+[\\s]?', '', x)\n",
        "\n",
        "  x = remove_punctuation(x)\n",
        "\n",
        "  x = x.encode(\"ascii\", \"ignore\").decode()\n",
        "\n",
        "  x = remove_stopwords(x)\n",
        "\n",
        "  x = lemmatize_words(x)    \n",
        "\n",
        "  x = \" \".join([word for word in x.split(' ')]) \n",
        "\n",
        "  x = \" \".join([word for word in x.split() if len(word)>2]) \n",
        "\n",
        "  return x\n",
        "\n",
        "def sentiment_scores(input_text):\n",
        " \n",
        "  obj = SentimentIntensityAnalyzer()\n",
        "  return obj.polarity_scores(input_text)\n",
        "\n",
        "def sentiment_category(input_dic):\n",
        "\n",
        "  if input_dic['compound'] >= 0.05 :\n",
        "    return 'Positive' \n",
        "  elif input_dic['compound'] <= - 0.05 :\n",
        "    return 'Negative'\n",
        "  else :\n",
        "    return 'Neutral'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JD_1z9XQg_pJ"
      },
      "source": [
        "Functions to fetch and process data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XQjukN1s7xHq"
      },
      "outputs": [],
      "source": [
        "def fetch_data(keywords, since, till):\n",
        "\n",
        "  dfs=[]\n",
        "  for i in keywords:\n",
        "    dfs.append(get_tweets(i, since, till))\n",
        "    \n",
        "  print([ len(i) for i in dfs])\n",
        "\n",
        "  twitterData = pd.concat(dfs)\n",
        "\n",
        "  yfinanceData = get_yfinance_data('VMW')\n",
        "  yfinanceData.Close = round(yfinanceData.Close,2)\n",
        "  yfinanceData = yfinanceData.reset_index()\n",
        "\n",
        "  return twitterData, yfinanceData\n",
        "\n",
        "def prepare_data():\n",
        "\n",
        "  keywords = ['vmware','esx','vmworld','vmc','vcf','vcenter']\n",
        "  since = '2020-01-01'\n",
        "  till = '2022-04-01'\n",
        "\n",
        "  start = datetime.datetime.strptime(since,'%Y-%m-%d')\n",
        "  end = datetime.datetime.strptime(till,'%Y-%m-%d')\n",
        "  daterange = [start + datetime.timedelta(days=x) for x in range(0, (end-start).days)]\n",
        "  base  = pd.DataFrame(daterange, columns=['Date'])\n",
        "\n",
        "  twitterData, yfinanceData =fetch_data(keywords, since, till)\n",
        "\n",
        "  twitterData['Date'] = twitterData.Datetime.values.astype(dtype='datetime64[D]') \n",
        "  twitterData['cleanText'] = twitterData.Text.apply(text_cleaning)\n",
        "  twitterData['words'] = twitterData['cleanText'].apply(lambda x: x.split())\n",
        "  twitterData['score'] = twitterData.cleanText.apply(sentiment_scores)\n",
        "  twitterData['sentiment'] = twitterData.score.apply(sentiment_category)\n",
        "  twitterData = pd.get_dummies(twitterData, columns=['sentiment'])\n",
        "  twitterData['sentiment'] = twitterData.score.apply(sentiment_category)\n",
        "\n",
        "  twitterWordCloud = twitterData[['Date','sentiment','words']].explode('words').groupby(['Date','sentiment','words']).size().reset_index(name='# occurence')\n",
        "\n",
        "  twitterAggData = twitterData.groupby('Date').agg({'sentiment_Negative':'sum', 'Tweet Id':'count'}).reset_index().rename(columns={'sentiment_Negative':'# neg tweets', 'Tweet Id':'# tweets'})\n",
        "\n",
        "  final_data  = pd.merge(base,yfinanceData[yfinanceData.Date>='2020-01-01'],on='Date',how='left')\n",
        "  final_data  = pd.merge(final_data,twitterAggData,on='Date',how='left')\n",
        "  final_data  = final_data[['Date', 'Close', '# neg tweets', '# tweets']]\n",
        "  final_data.rename(columns={'Close':'Stock Price'}, inplace=True)\n",
        "  final_data.fillna(0)\n",
        "  final_data['% neg tweets'] = round( final_data['# neg tweets'] / final_data['# tweets'] , 2)\n",
        "  final_data['# neg tweets in LXdays'] = final_data['# neg tweets'].rolling(7).sum().fillna(0)\n",
        "  final_data['# tweets in LXdays'] = final_data['# tweets'].rolling(7).sum()\n",
        "  final_data['% neg tweets in LXdays'] = round( final_data['# neg tweets in LXdays'] / final_data['# tweets in LXdays'] , 2)\n",
        "  final_data['Change in stock price'] = final_data['Stock Price'] - final_data['Stock Price'].shift(1)\n",
        "  final_data['Last 7 days avg stock price'] = final_data['Stock Price'].rolling(window=7,min_periods=1, closed='left').mean()\n",
        "  final_data['Change in Last 7 days avg stock price'] = final_data['Last 7 days avg stock price'] - final_data['Last 7 days avg stock price'].shift(1)\n",
        "  final_data.dropna(inplace=True)\n",
        "\n",
        "\n",
        "  X = final_data[['Last 7 days avg stock price', '# tweets', '% neg tweets', '% neg tweets in LXdays', 'Change in stock price', 'Change in Last 7 days avg stock price']]\n",
        "  y = final_data['Stock Price']\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, \n",
        "                                                      y,\n",
        "                                                      test_size=0.2, \n",
        "                                                      shuffle=False)\n",
        "  print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
        "  model = LinearRegression()\n",
        "  model.fit(X_train,y_train)\n",
        "  print('Logistic Regression Created')\n",
        "  y_pred = model.predict(X)\n",
        "\n",
        "  final_data['predicted_value'] = y_pred\n",
        "\n",
        "  return twitterWordCloud,final_data\n",
        "\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OP2xT1qdeJ1i"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "  twitterWordCloud,final_data = prepare_data()\n",
        "  twitterWordCloud.to_csv('word_cloud.csv',index=False)\n",
        "  final_data.to_csv('final_data.csv',index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "r_1UF2ELgJfE",
        "outputId": "575432eb-4550-437d-f524-6cf6e4737ca7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1815, 1539, 33, 773, 28053, 30]\n",
            "(356, 6) (90, 6) (356,) (90,)\n",
            "Logistic Regression Created\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "  main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "SentimentAnalysis.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}